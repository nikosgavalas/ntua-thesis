import unittest
import os.path

import numpy as np
from sklearn.metrics import f1_score

from adr.batch.iforest import IsolationForest


class Test(unittest.TestCase):

    forest = IsolationForest()

    # get the path relative to this script
    dirname = os.path.dirname(__file__)
    labeled_set = np.loadtxt(
        os.path.join(dirname, '../../../data/tests/set_255_45_2_l.csv'),
        delimiter=','
    )

    X = labeled_set[:, :2]
    y = labeled_set[:, 2]

    def test_0_training(self):
        self.forest.fit(self.X)
        # we 're happy if no exceptions are thrown

    def test_1_serialization(self):
        self.forest.save_to_disk()
        self.assertTrue(os.path.isfile('model.json'))
        self.forest.load_from_disk()
        # delete the file after running the test
        os.remove('model.json')

    def test_2_anomaly_scores(self):
        # pass the eval dataset unlabeled
        actual = self.forest.score(self.X)
        #print(np.array2string(actual, separator=', ', precision=100))
        exp = np.array(
            [0.4701785366497139, 0.4030565305413713, 0.4055107663505487,
             0.3775960665389644, 0.4206473389389227, 0.36407687795909205,
             0.4561562132780457, 0.37328990339098406, 0.3718926100879106,
             0.3864914251893288, 0.400271025959121, 0.4662551110036713,
             0.3873766998100659, 0.3928365452191849, 0.38234256032176334,
             0.4611092832661859, 0.40840004894442805, 0.4650834551432075,
             0.3685234868614938, 0.4067930945620097, 0.43948290324384554,
             0.40004935043871587, 0.37442690284089497, 0.37674168684589593,
             0.4509819906216935, 0.4012017131683136, 0.415171421381363,
             0.4232614575480883, 0.4145852050179146, 0.44785517078581727,
             0.48563400930130557, 0.5061306504041603, 0.37414535049697706,
             0.4119332621555585, 0.4627443306430444, 0.3863236604988162,
             0.3873208720924968, 0.4375048974678098, 0.4496393191506192,
             0.4828299642329394, 0.40825245914740116, 0.3742818636974962,
             0.3794578274021096, 0.4094331486111166, 0.36677521860403584,
             0.43703893995278786, 0.4803162041326229, 0.39441273630339163,
             0.3670424673751498, 0.4088721606585407, 0.3776768819626415,
             0.37093694679982103, 0.3829072985902071, 0.4477713659971415,
             0.39674372446411055, 0.4687903728981518, 0.3770428055365743,
             0.46342470225376936, 0.38602203226740633, 0.38740127520130446,
             0.42738413969462685, 0.39817567466469045, 0.3987765119385737,
             0.4643623490810729, 0.38288589723051814, 0.42857049429072447,
             0.4007144709817014, 0.4178817381323398, 0.38190411068851526,
             0.38165517540000266, 0.4015561991261248, 0.4662431908315278,
             0.48095604457364965, 0.44012001987412525, 0.39970267554473116,
             0.37608598324155373, 0.41838557064660864, 0.4544189977714999,
             0.40635636283123966, 0.413932638568329, 0.4100935658235073,
             0.4560465066561125, 0.4058377789225478, 0.48823388969374487,
             0.4233311642710245, 0.385190722601646, 0.3914096464417903,
             0.42416634757439026, 0.37095537717969923, 0.36841418629018063,
             0.40187069066208597, 0.4313917301794314, 0.3842131650647718,
             0.3790579047211542, 0.38181572626577637, 0.40428566197700333,
             0.384726335072641, 0.37246321756082607, 0.4899995694236714,
             0.37311052875859346, 0.3644735471361522, 0.4147852193626427,
             0.3789975965467876, 0.40302939179009095, 0.42837218028327617,
             0.4003353843869864, 0.379289297363332, 0.39954300819915867,
             0.38522140772866303, 0.3973412487598612, 0.409466712334477,
             0.3828073059852696, 0.45532228759696985, 0.37910369297100643,
             0.40927168665485536, 0.39465145020784026, 0.4304337229722287,
             0.4329106986015835, 0.3776339398793723, 0.37160324558441854,
             0.3914724051527423, 0.36977991565351, 0.4045376555074998,
             0.41088146435886536, 0.43628125984662525, 0.40736488632931855,
             0.4051678074404988, 0.39179764354234947, 0.3702695266384821,
             0.44461717688256264, 0.4050086195974025, 0.4513854665594367,
             0.3941885208539282, 0.4755718493227222, 0.3744586362160721,
             0.40540385984413535, 0.3707850657469644, 0.39803687143214395,
             0.4317207840835818, 0.4058534753489352, 0.4198710933158499,
             0.3667494754177939, 0.41043796040347214, 0.44483223735394295,
             0.433119917029634, 0.37521715874499195, 0.4592715508869497,
             0.3737366626656744, 0.41030684218045427, 0.4309586244793906,
             0.3642912083967966, 0.36810906512627867, 0.37008720119591365,
             0.4101375145994276, 0.37928553406839866, 0.43989145040781324,
             0.41163718578332625, 0.42955566269402645, 0.39717940233861804,
             0.41709218214470284, 0.37434065035861985, 0.37363934844804697,
             0.3811466220188309, 0.41698412010337715, 0.44835785508742626,
             0.3849157340334975, 0.4102385175416598, 0.3928235341809566,
             0.4056525280194997, 0.3702315885474401, 0.37946589039620343,
             0.3953109912764659, 0.40947717505815595, 0.4780330502995694,
             0.46955575455458304, 0.4223608466059294, 0.4290832332284657,
             0.37847391267078206, 0.36710893893759333, 0.3880469969998996,
             0.36733072877018136, 0.4460363934876196, 0.40324121792675116,
             0.39731802335251487, 0.4840611082901944, 0.45018043681460557,
             0.4113233035120823, 0.38275575715437293, 0.46904753554876955,
             0.41447388295512705, 0.38393090607455505, 0.39589366314862534,
             0.5218043142051227, 0.409702284134296, 0.45675272525320254,
             0.3817692493839755, 0.47805709608627744, 0.3988107509327507,
             0.401424976940015, 0.3806912808373247, 0.420548719516121,
             0.37634349163414427, 0.3696219465705118, 0.36775431422019206,
             0.40309350044329734, 0.3843913455329989, 0.44223449363354406,
             0.37898895068133087, 0.38039907319445687, 0.36754241721376785,
             0.38146311370347263, 0.40089479657281374, 0.36862653461909256,
             0.41305749386917495, 0.44374967196069603, 0.3836602127491796,
             0.38422435431007723, 0.4471949022597997, 0.3662997237625912,
             0.41750668503025873, 0.4162268162015848, 0.43868467596576805,
             0.3704231711037425, 0.3690515382717885, 0.4002971089928693,
             0.4080234496775221, 0.4380257100824908, 0.37744189854061294,
             0.3691724784265035, 0.36922137325959525, 0.4170308586694204,
             0.4889198852771617, 0.4435932259415328, 0.4868700525196739,
             0.4172338211565787, 0.4420201949630191, 0.5319364463483771,
             0.40503178157603287, 0.4112847530568195, 0.3801686922599512,
             0.45659382187278963, 0.3726464232267764, 0.38144197661910884,
             0.42476222167902306, 0.4678261011759076, 0.40959104195372764,
             0.3959608657791632, 0.436483728767057, 0.40836087184478437,
             0.3834566798052549, 0.4915179573743521, 0.4003839790103386,
             0.39051479941836986, 0.4072111937855327, 0.3725288365776472,
             0.6685773793986064, 0.5163801432439235, 0.5058683387479751,
             0.6251272891218125, 0.6257546739433105, 0.6197116155750222,
             0.6115411918366981, 0.5704099844384256, 0.6625328956960442,
             0.5494523346162747, 0.5231842084780778, 0.37076074732411407,
             0.638856933890969, 0.4189817143719965, 0.5954317872352249,
             0.5997491504378689, 0.4208046277933096, 0.6175197823107611,
             0.6507801169514882, 0.5919428118604877, 0.5636587432417942,
             0.5584552952685886, 0.5787812814607716, 0.5818684197594959,
             0.5122145057809577, 0.5167039568802173, 0.5658171891833542,
             0.6377625920880962, 0.6319809872416131, 0.5461128535547686,
             0.5843085499656109, 0.43661772795102477, 0.5276826284537013,
             0.5025364660670574, 0.6080806923260824, 0.5488542116004409,
             0.6323454750302306, 0.5069113887596984, 0.6009634674667181,
             0.631755121217368, 0.5004089472660858, 0.5287858150171301,
             0.5525008793450797, 0.5480923558474607, 0.5357384456839536]
        )
        self.assertTrue(np.array_equal(actual, exp))

    def test_3_predict_with_contamination(self):
        actual = self.forest.predict(self.X, 0.1)
        exp = np.array(
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,
             1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0.,
             0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.]
        )
        self.assertTrue(np.array_equal(actual, exp))

    def test_4_predict_without_contamination(self):
        actual = self.forest.predict(self.X)
        exp = np.array(
            [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
             0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,
             1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,
             1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]
        )
        self.assertTrue(np.array_equal(actual, exp))

    def test_5_f1_score(self):
        preds = self.forest.predict(self.X)
        exp = 0.9213483146067416
        self.assertEqual(f1_score(self.y, preds), exp)


if __name__ == "__main__":
    unittest.main()
